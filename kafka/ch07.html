<?xml version="1.0" encoding="utf-8" ?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter&#xA0;7.&#xA0;Kafka Integrations</title><link rel="stylesheet" href="epub.css" type="text/css"/><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/></head><body id="page"><div class="chapter" title="Chapter&#xA0;7.&#xA0;Kafka Integrations"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter&#xA0;7.&#xA0;Kafka Integrations</h1></div></div></div><p>Consider a use case for a website where continuous security events, such as user authentication and authorization to access secure resources need to be tracked, and decisions need to be taken in real time for any security breach. Using any typical batch-oriented data processing systems, such as Hadoop, where all the data needs to be collected first and then processed to find out patterns, will make it too late to decide whether there is any security threat to the web application or not. Hence, this is the classical use case for real-time data processing.</p><p>Let's consider another use case, where raw clickstreams generated by customers through website usage are captured and preprocessed. Processing these clickstreams provides valuable insight into customer preferences and these insights can be coupled later with marketing campaigns and recommendation engines to offer an analysis of consumers. Hence, we can simply say that this large amount of clickstream data stored on Hadoop will get processed by Hadoop MapReduce jobs in batch mode, not in real time.</p><p>In this chapter, we shall be exploring how Kafka can be integrated with the following technologies to address different use cases, such as real-time processing using Storm and batch processing using Hadoop:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Kafka integration with Storm</li><li class="listitem" style="list-style-type: disc">Kafka integration with Hadoop</li></ul></div><p>So let's start.</p><div class="section" title="Kafka integration with Storm"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec33"/>Kafka integration with Storm</h1></div></div></div><p>Processing<a id="id175" class="indexterm"/> small amount of data at real time was never a challenge using technologies, such as the <span class="strong"><strong>Java Messaging Service</strong></span> (<span class="strong"><strong>JMS</strong></span>)<a id="id176" class="indexterm"/>; however, these processing systems show performance limitations while dealing with large volumes of streaming data. Also, these systems are not good scalable solutions.</p><div class="section" title="Introduction to Storm"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec34"/>Introduction to Storm</h2></div></div></div><p>
<span class="strong"><strong>Storm</strong></span> is<a id="id177" class="indexterm"/> an open source, distributed, reliable, and fault-tolerant system for processing streams of large volumes of data in real time. It supports many use cases, such as real-time analytics, online machine learning, continuous computation, and <a id="id178" class="indexterm"/>
<span class="strong"><strong>Extract Transformation Load</strong></span> (<span class="strong"><strong>ETL</strong></span>) paradigm.</p><p>There are various components<a id="id179" class="indexterm"/> that work together for streaming data processing, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spout</strong></span>: This <a id="id180" class="indexterm"/>is <a id="id181" class="indexterm"/>a source of stream, which is a continuous stream of log data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bolt</strong></span>: The<a id="id182" class="indexterm"/> spout passes the data to a component called <span class="strong"><strong>bolt</strong></span><a id="id183" class="indexterm"/>. A bolt consumes any number of input streams, does some processing, and possibly emits new streams. For example, emitting a stream of trend analysis by processing a stream of tweets.</li></ul></div><p>The following diagram shows spout and bolt in the Storm <a id="id184" class="indexterm"/>architecture:</p><div class="mediaobject"><img src="graphics/7938OS_07_01.jpg" alt="Introduction to Storm"/></div><p>We can assume a Storm cluster to be a chain of bolt components, where each bolt performs some kind of transformation on the data streamed by the spout.</p><p>Next in the Storm cluster, jobs are typically referred to as <span class="strong"><strong>topologies</strong></span>
<a id="id185" class="indexterm"/>; the only difference is that these topologies run forever. For real-time computation on Storm, topologies, which are nothing but graphs of computation, are created. Typically, topologies define how data will flow from spouts through bolts. These topologies<a id="id186" class="indexterm"/> can be transactional or non-transactional in nature.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>Complete information about Storm can be found at <a class="ulink" href="http://storm-project.net/">http://storm-project.net/</a>.</p></div></div><p>The following section is useful if you have worked with Storm or have working knowledge of Storm.</p></div><div class="section" title="Integrating Storm"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec35"/>Integrating Storm</h2></div></div></div><p>We have <a id="id187" class="indexterm"/>already learned in the previous chapters that Kafka<a id="id188" class="indexterm"/> is a high-performance publisher-subscriber-based messaging system with highly scalable properties. Kafka Spout is available for integrating Storm with Kafka clusters.</p><p>The <a id="id189" class="indexterm"/>source code for Kafka Storm Spout<a id="id190" class="indexterm"/> is available at <a class="ulink" href="https://github.com/nathanmarz/storm-contrib/tree/master/storm-kafka">https://github.com/nathanmarz/storm-contrib/tree/master/storm-kafka</a>.</p><p>The Kafka Spout is a regular spout implementation that reads the data from a Kafka cluster. It requires the following parameters<a id="id191" class="indexterm"/> to get connected to the Kafka cluster:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">List of Kafka brokers</li><li class="listitem" style="list-style-type: disc">Number of partitions per host</li><li class="listitem" style="list-style-type: disc">Topic name used to pull the message</li><li class="listitem" style="list-style-type: disc">Root path in ZooKeeper, where Spout stores the consumer offset</li><li class="listitem" style="list-style-type: disc">ID for the consumer required for storing the consumer offset in ZooKeeper</li></ul></div><p>The following code sample shows the <code class="literal">KafkaSpout</code> class instance initialization with the previous parameters:</p><div class="informalexample"><pre class="programlisting">SpoutConfig spoutConfig = new SpoutConfig(
  ImmutableList.of("localhost:9092", "localhost:9093"), 
  2, 
  " othertopic", 
  "/kafkastorm", 
  "consumID"); 
KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);</pre></div><p>The Kafka Spout uses ZooKeeper to store the states of the message offset and segment consumption tracking if it is consumed. These offsets are stored at the root path specified for the ZooKeeper. By default, Storm uses its own ZooKeeper cluster for storing the message offset, but other ZooKeeper clusters can also be used by setting it in Spout configuration. As per the design, Spout works in a single-threaded model, as all the parallelism is handled by the Storm cluster. It also has a provision to rewind to a previous offset rather than starting from the last saved offset.</p><p>Kafka Spout also provides an option to specify how Spout fetches messages from a Kafka cluster by setting properties, such as buffer sizes and timeouts.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>To run Kafka with Storm, clusters for both Storm and Kafka need to be set up and should be running. A Storm cluster setup is out of the scope of this book.</p></div></div><p>The <a id="id192" class="indexterm"/>following <a id="id193" class="indexterm"/>diagram shows the high-level integration view of what a Kafka Storm working model will look like:</p><div class="mediaobject"><img src="graphics/7938OS_07_02.jpg" alt="Integrating Storm"/></div></div></div></div></body></html>
