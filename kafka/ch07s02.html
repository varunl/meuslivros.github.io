<?xml version="1.0" encoding="utf-8" ?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Kafka integration with Hadoop</title><link rel="stylesheet" href="epub.css" type="text/css"/><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/></head><body id="page"><div class="section" title="Kafka integration with Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec34"/>Kafka integration with Hadoop</h1></div></div></div><p>Resource<a id="id1940" class="indexterm"/> sharing, stability, availability, and scalability are a few of the many challenges of distributed computing. Nowadays, an additional challenge is to process extremely large volumes of data in TBs or PBs.</p><div class="section" title="Introduction to Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec36"/>Introduction to Hadoop</h2></div></div></div><p>Hadoop is a<a id="id1950" class="indexterm"/> large-scale distributed batch processing framework which parallelizes data processing across many nodes and addresses the challenges for distributed computing, including big data.</p><p>Hadoop works on the principle of the MapReduce framework (introduced by Google), which provides a simple interface for the parallelization and distribution of large-scale computations. Hadoop has its own distributed data filesystem called <span class="strong"><strong>HDFS</strong></span> (<span class="strong"><strong>Hadoop Distributed File System</strong></span>)<a id="id1960" class="indexterm"/>. In any typical Hadoop cluster, HDFS splits the data into small pieces (called <span class="strong"><strong>blocks</strong></span>
<a id="id1970" class="indexterm"/>) and distributes it to all the nodes. HDFS also creates the replication of these small pieces of data and stores it to make sure that if any node is down, the data is available from another node.</p><p>The following diagram shows the high-level view of a <a id="id1980" class="indexterm"/>multinode Hadoop cluster:</p><div class="mediaobject"><img src="graphics/7938OS_07_03.jpg" alt="Introduction to Hadoop"/></div><p>Hadoop has the following main <a id="id1990" class="indexterm"/>components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Name Node</strong></span>: This<a id="id2000" class="indexterm"/> is a single point of interaction for <a id="id2010" class="indexterm"/>HDFS. A name node stores the information about the small pieces (blocks) of data that are distributed across the node.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Secondary Name Node</strong></span>: This <a id="id2020" class="indexterm"/>node stores <a id="id2030" class="indexterm"/>the edit logs, which are helpful to restore the latest updated state of HDFS in case of a name node failure.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data Node</strong></span>: These<a id="id2040" class="indexterm"/> nodes <a id="id2050" class="indexterm"/>store the actual data distributed by the name node in blocks and also store the replicated copy of data from other nodes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Job Tracker</strong></span>: This<a id="id2060" class="indexterm"/> is<a id="id2070" class="indexterm"/> responsible for splitting the MapReduce jobs into smaller tasks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Task Tracker</strong></span>: The<a id="id2080" class="indexterm"/> task tracker is responsible for<a id="id2090" class="indexterm"/> the execution of tasks split by the job tracker.</li></ul></div><p>The data nodes and the task tracker share the same machines, MapReduce jobs split, and execution of tasks is done based on the data store location information provided by the name node.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>Complete information about Hadoop can be found at <a class="ulink" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a>.</p></div></div></div><div class="section" title="Integrating Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec37"/>Integrating Hadoop</h2></div></div></div><p>This <a id="id210" class="indexterm"/>section<a id="id211" class="indexterm"/> is useful if you have worked with Hadoop or have working knowledge of Hadoop.</p><p>For <a id="id212" class="indexterm"/>real-time publish-subscribe use cases, Kafka is used to build a pipeline that is available for real-time processing or monitoring and to load the data into Hadoop, NoSQL, or data warehousing systems for offline processing and reporting.</p><p>Kafka provides the source code for both the Hadoop producer and consumer, under its <code class="literal">contrib</code> directory.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>This section only discusses the source code provided with Kafka codebases for Hadoop; no other third-party solution for Kafka and Hadoop integrations is discussed.</p></div></div></div><div class="section" title="Hadoop producer"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec38"/>Hadoop producer</h2></div></div></div><p>A <a id="id213" class="indexterm"/>Hadoop producer provides a bridge for publishing the data from a Hadoop cluster to Kafka, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/7938OS_07_04.jpg" alt="Hadoop producer"/></div><p>For a Kafka producer, Kafka topics are considered as URIs, and to connect to a specific Kafka broker, URIs are specified as follows:</p><div class="informalexample"><pre class="programlisting">kafka://&lt;kafka-broker&gt;/&lt;kafka-topic&gt;</pre></div><p>The Hadoop producer code suggests two possible approaches<a id="id214" class="indexterm"/> for getting the data from Hadoop:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Using the Pig script and writing messages in Avro format</strong></span>: In this approach, Kafka<a id="id215" class="indexterm"/> producers use <span class="strong"><strong>Pig</strong></span> scripts for writing data in a binary <span class="strong"><strong>Avro</strong></span> format, where each row signifies a single message. For pushing the data into the Kafka cluster, the <code class="literal">AvroKafkaStorage</code> class (extends Pig's <code class="literal">StoreFunc</code> class) takes the Avro schema as its first argument and connects to the Kafka URI. Using the <code class="literal">AvroKafkaStorage</code> producer<a id="id216" class="indexterm"/>, we can also easily write to multiple topics and brokers in the same Pig script-based job.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Using the Kafka OutputFormat class for jobs</strong></span>: In <a id="id217" class="indexterm"/>this approach, the Kafka <code class="literal">OutputFormat</code> class (extends Hadoop's <code class="literal">OutputFormat</code> class) is used for publishing data to the Kafka cluster. This approach publishes messages as bytes and provides control over output by using low-level methods of publishing. The<a id="id218" class="indexterm"/> Kafka <code class="literal">OutputFormat</code> class uses the <code class="literal">KafkaRecordWriter</code> class (extends Hadoop's <code class="literal">RecordWriter</code> class) for writing a record (message) to a Hadoop cluster.</li></ul></div><p>For Kafka producers, we can also configure Kafka producer parameters and Kafka broker information under a job's configuration.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>For more detailed usage of the Kafka producer, refer to <code class="literal">README</code> under the <code class="literal">Kafka-0.8/contrib/hadoop-producer</code> directory.</p></div></div></div><div class="section" title="Hadoop consumer"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec39"/>Hadoop consumer</h2></div></div></div><p>A <a id="id219" class="indexterm"/>Hadoop consumer is a Hadoop job that pulls data from the Kafka broker and pushes it into HDFS. The following diagram shows the position of a Kafka consumer in <a id="id220" class="indexterm"/>the architecture pattern:</p><div class="mediaobject"><img src="graphics/7938OS_07_05.jpg" alt="Hadoop consumer"/></div><p>A Hadoop job<a id="id221" class="indexterm"/> performs parallel loading from Kafka to HDFS, and the number of mappers for loading the data depends on the number of files in the input directory. The output directory contains data coming from Kafka and the updated topic offsets. Individual mappers write the offset of the last consumed message to HDFS at the end of the map task. If a job fails and jobs get restarted, each mapper simply restarts from the offsets stored in HDFS.</p><p>The ETL example provided in the <code class="literal">Kafka-0.8/contrib/hadoop-consumer</code> directory demonstrates the extraction of Kafka data and loading it to HDFS.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>For more information on the detailed usage of a Kafka consumer, refer to <code class="literal">README</code> under the <code class="literal">Kafka-0.8/contrib/hadoop-consumer</code> directory.</p></div></div></div></div></body></html>
